{
 "cells": [
  {
   "cell_type": "raw",
   "id": "64cf96a1-46e1-4b62-a37c-c9ed65e4cb1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T00:35:00.552864Z",
     "iopub.status.busy": "2023-11-07T00:35:00.552864Z",
     "iopub.status.idle": "2023-11-07T00:35:00.576403Z",
     "shell.execute_reply": "2023-11-07T00:35:00.573396Z",
     "shell.execute_reply.started": "2023-11-07T00:35:00.552864Z"
    },
    "id": "ee881faf-a1d3-4f46-b59b-ed4d80f503ac"
   },
   "source": [
    "# Assignment 2C: N-gram Text Correction using Natural Language Processing\n",
    "\n",
    "## DTSC-685: Natural Language Processing\n",
    "\n",
    "## Name: [Gilbert Morgan]\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "In the quiet town of Stratford-upon-Avon, a long-forgotten manuscript bearing the mark of William Shakespeare was unearthed in the depths of an ancient library. The discovery sent waves through the literary world, but joy turned to dismay as scholars realized parts of the script had been lost to time and poor storage. In an unprecedented effort, data scientists from a renowned university gathered all known works of Shakespeare. Using the art of Natural Language Processing, they deployed n-gram and fivegram models to predict and weave in the missing words, aiming to restore the manuscript to its intended glory.\n",
    "\n",
    "- WS_train.txt - All WS works.\n",
    "\n",
    "- WS_test.txt - The manuscript, with the words lost marked as `<DELETED>`.\n",
    "    \n",
    "- WS_validation - Text to validade our models performance.\n",
    "    \n",
    "    \n",
    "**Objective**: To implement an N-gram language model that can predict missing words in a text corpus, using the works of William Shakespeare as a basis for model training and evaluation.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Data Preparation**: Load and preprocess the training and test texts, preparing them for the N-gram model training.\n",
    "2. **N-gram Model Training - parts A and B**: Utilize the `WS_train.txt` to create an N-gram language model that learns the word sequences and their probabilities.\n",
    "3. **Text Correction**: Apply the trained N-gram model to predict and fill in the `<DELETED>` placeholders in `WS_test.txt`.\n",
    "4. **Evaluation**: Compare the corrected text against `WS_validation.txt` to evaluate the accuracy of the N-gram model's predictions.\n",
    "5. **Export Models for codegrade evaluation**: Export bigram and fivegram models using appropriate method for Codegrade evaluation\n",
    "\n",
    "By the end of this assignment, you will have a deeper understanding of how N-gram models can be used for text prediction and correction, and you'll be equipped with practical experience in evaluating NLP model performance.\n",
    "\n",
    "Please note that just because your code does not result in an error, does not mean that it is correct. You should use the provided examples to test your code before submission. Pass the example input to your function and see if the output your code provides matches the example output *exactly*. This is how CodeGrade will assess your code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d047d058-2fcf-40d3-8e24-7e69d6e246bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3142010f-2d7f-4f0c-bbae-6a9a6a571306",
   "metadata": {
    "id": "3142010f-2d7f-4f0c-bbae-6a9a6a571306"
   },
   "source": [
    "### 1 - Data Preparation\n",
    "\n",
    "Proper data preparation is essential for training an effective N-gram model. We will define specific functions for loading, cleaning, generating N-grams, and building a vocabulary from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caabf810-d33b-4327-a327-c71171f80ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eddie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from DTSC_685_Assignment2C_NGram_only_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38694b6-2ee9-4412-b9b5-698de512c961",
   "metadata": {
    "id": "b38694b6-2ee9-4412-b9b5-698de512c961"
   },
   "source": [
    "#### 1.1 - Import necessary libraries:\n",
    "\n",
    "\n",
    ">```python\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b464e-c13f-48f0-aed0-7d044e1cf4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c91b83-9441-461d-9829-429d6fac993c",
   "metadata": {
    "id": "b1c91b83-9441-461d-9829-429d6fac993c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29023\n",
      "Number of 5-grams: 1130241\n",
      "\n",
      "Example 5-grams:\n",
      "('1609', 'the', 'sonnets', 'by', 'william')\n",
      "('the', 'sonnets', 'by', 'william', 'shakespeare')\n",
      "('sonnets', 'by', 'william', 'shakespeare', '1')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        except:\n",
    "            import nltk\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('punkt')\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def load_text(self, filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def clean_text(self, text, remove_stopwords=False, remove_punctuation=True):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text) if remove_punctuation else text\n",
    "        tokens = word_tokenize(text)\n",
    "        if remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def generate_ngrams(self, text, n=5):\n",
    "        tokens = word_tokenize(text)\n",
    "        return list(ngrams(tokens, n))\n",
    "    \n",
    "    def build_vocabulary(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        return set(tokens)\n",
    "\n",
    "def prepare_shakespeare_data():\n",
    "    preprocessor = TextPreprocessor()\n",
    "    train_text = preprocessor.load_text('WS_train.txt')\n",
    "    test_text = preprocessor.load_text('WS_test.txt')\n",
    "    validation_text = preprocessor.load_text('WS_validation.txt')\n",
    "    clean_train = preprocessor.clean_text(\n",
    "        train_text,\n",
    "        remove_stopwords=False,\n",
    "        remove_punctuation=False\n",
    "    )\n",
    "    vocab = preprocessor.build_vocabulary(clean_train)\n",
    "    fivegrams = preprocessor.generate_ngrams(clean_train, n=5)\n",
    "    return {\n",
    "        'train_text': clean_train,\n",
    "        'test_text': test_text,\n",
    "        'validation_text': validation_text,\n",
    "        'vocabulary': vocab,\n",
    "        'fivegrams': fivegrams\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = prepare_shakespeare_data()\n",
    "    print(f\"Vocabulary size: {len(data['vocabulary'])}\")\n",
    "    print(f\"Number of 5-grams: {len(data['fivegrams'])}\")\n",
    "    print(\"\\nExample 5-grams:\")\n",
    "    for gram in list(data['fivegrams'])[:3]:\n",
    "        print(gram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad37c3-d98e-4bca-bd1c-743a66747661",
   "metadata": {
    "id": "4aad37c3-d98e-4bca-bd1c-743a66747661"
   },
   "source": [
    "#### 1.2 - Load the Text\n",
    "\n",
    "Define a function called `load_text` that reads a text file and returns its contents as a string. The function should take the following parameters:\n",
    "\n",
    "    - `file_path`: A string representing the path to the text file to be read.\n",
    "\n",
    "\n",
    "Use Python's built-in `open` function for reading files with appropriate error handling (for cases where the file might not exist) and `encoding='utf-8'`.\n",
    "\n",
    "Use the `load_text` function to load the `WS_train.txt` as the training text data. Store it as `train_text`.\n",
    "\n",
    "\n",
    "**Expected Outuput:**\n",
    "\n",
    ">```python\n",
    "train_text[0:500]\n",
    "\n",
    "\n",
    "    \"1609\\n\\nTHE SONNETS\\n\\nby William Shakespeare\\n\\n\\n\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only heral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d78d462-bcc1-4793-9e20-d292db6ab10d",
   "metadata": {
    "id": "5d78d462-bcc1-4793-9e20-d292db6ab10d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_text[0:500]:\n",
      "\"1609\\n\\nTHE SONNETS\\n\\nby William Shakespeare\\n\\n\\n\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only heral\"\n"
     ]
    }
   ],
   "source": [
    "def load_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except UnicodeDecodeError:\n",
    "        raise UnicodeDecodeError(f\"Error: Unable to decode '{file_path}'. Please ensure the file is UTF-8 encoded.\")\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"Error reading '{file_path}': {str(e)}\")\n",
    "\n",
    "try:\n",
    "    train_text = load_text('WS_train.txt')\n",
    "    print(\"train_text[0:500]:\")\n",
    "    print(repr(train_text[0:500]))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b8da2-ac99-4c28-94a2-5dd086906358",
   "metadata": {
    "id": "445b8da2-ac99-4c28-94a2-5dd086906358"
   },
   "source": [
    "#### 1.3 - Clean the Text\n",
    "\n",
    "Define a function named `clean_text` that will standardize, tokenize, and remove punctuation from the text data, while retaining the **<DELETED>** placeholders. The function should take the following parameter:\n",
    "\n",
    "- `text`: The raw string of text to be cleaned.\n",
    "\n",
    "The function will:\n",
    "    \n",
    "1. Convert the text to lowercase.\n",
    "2. Tokenize\n",
    "3. Remove all punctuation tokens using `string.punctuation`.\n",
    "4. Remove stop words tokens using NLTK's `stopwords.words('english')`\n",
    "*These things should be done in the order specified above.*\n",
    "    \n",
    "The function `clean_text` should return a **list** of clean tokens, retaining the **<DELETED>** placeholders.\n",
    "\n",
    "Ps.:\n",
    "Don't forget to:\n",
    "        \n",
    "Apply the `clean_text` function to the loaded `train_text`. Since the function internally handles the removal of stop words and punctuation (except for the `<DELETED>` placeholders), only the raw text needs to be passed as an argument. The output should be stored in a variable named `cleaned_train_text`.\n",
    "\n",
    ">```python\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "**Expected Outuput:**\n",
    "\n",
    ">```python\n",
    "cleaned_train_text[0:30]\n",
    "    ['1609',\n",
    "     'sonnets',\n",
    "     'william',\n",
    "     'shakespeare',\n",
    "     '1',\n",
    "     'fairest',\n",
    "     'creatures',\n",
    "     'desire',\n",
    "     'increase',\n",
    "     'thereby',\n",
    "     'beauty',\n",
    "     \"'s\",\n",
    "     'rose',\n",
    "     'might',\n",
    "     'never',\n",
    "     'die',\n",
    "     'riper',\n",
    "     'time',\n",
    "     'decease',\n",
    "     'tender',\n",
    "     'heir',\n",
    "     'might',\n",
    "     'bear',\n",
    "     'memory',\n",
    "     'thou',\n",
    "     'contracted',\n",
    "     'thine',\n",
    "     'bright',\n",
    "     'eyes',\n",
    "     \"feed'st\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4b3f09-4885-48cc-92d0-6f2ccee29263",
   "metadata": {
    "id": "4c4b3f09-4885-48cc-92d0-6f2ccee29263"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eddie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eddie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_train_text[0:30]\n",
      "[   '1609',\n",
      "    'sonnets',\n",
      "    'william',\n",
      "    'shakespeare',\n",
      "    '1',\n",
      "    'fairest',\n",
      "    'creatures',\n",
      "    'desire',\n",
      "    'increase',\n",
      "    'thereby',\n",
      "    'beauty',\n",
      "    \"'s\",\n",
      "    'rose',\n",
      "    'might',\n",
      "    'never',\n",
      "    'die',\n",
      "    'riper',\n",
      "    'time',\n",
      "    'decease',\n",
      "    'tender',\n",
      "    'heir',\n",
      "    'might',\n",
      "    'bear',\n",
      "    'memory',\n",
      "    'thou',\n",
      "    'contracted',\n",
      "    'thine',\n",
      "    'bright',\n",
      "    'eyes',\n",
      "    \"feed'st\"]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    punct_set = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == '<deleted>':\n",
    "            cleaned_tokens.append(token)\n",
    "        elif all(char in punct_set for char in token):\n",
    "            continue\n",
    "        elif token in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_tokens.append(token)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "try:\n",
    "    with open('WS_train.txt', 'r', encoding='utf-8') as file:\n",
    "        train_text = file.read()\n",
    "    \n",
    "    cleaned_train_text = clean_text(train_text)\n",
    "    \n",
    "    print(\"cleaned_train_text[0:30]\")\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cleaned_train_text[0:30])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddfb7e-1427-4ccc-bd73-9cb751aa6bd3",
   "metadata": {
    "id": "a1ddfb7e-1427-4ccc-bd73-9cb751aa6bd3"
   },
   "source": [
    "#### 1.4 - Create N-grams\n",
    "\n",
    "Define a function named `create_ngrams` that will convert a list of tokens into a list of N-grams. The function should take the following parameters:\n",
    "\n",
    "- `tokens`: A list of words (tokens) from which to create N-gram\n",
    "\n",
    "- `n`: The order of the N-gram (e.g., 2 for bigrams, 3 for trigrams, etc.\n",
    "\n",
    "\n",
    "Use the `ngrams` function from NLTK to create N-grams from tokens.\n",
    "Introduce special tokens `<s>` and `</s>` to indicate the start and the end of the text. You should have `n-1` special token at the beginning of your text and only 1 special token at the end of your text.\n",
    "\n",
    "\n",
    "The function should return a **list** of N-grams.\n",
    "\n",
    "\n",
    "Use the `create_ngrams` function to convert `cleaned_train_text` into two sets of N-grams:\n",
    "\n",
    "- Create bigrams from `cleaned_train_text` and store them in a variable named `train_bigrams` by passing `cleaned_train_text` with the appropriate value of `n`.    \n",
    "\n",
    "- Create fivegrams from `cleaned_train_text` and store them in a variable named `train_fivegrams` by passing `cleaned_train_text` with the appropriate value of `n`.\n",
    "\n",
    "**Expected Outuput:**\n",
    "\n",
    ">```python\n",
    "train_bigrams[0:15]\n",
    "\n",
    "    [('<s>', '1609'),\n",
    "     ('1609', 'sonnets'),\n",
    "     ('sonnets', 'william'),\n",
    "     ('william', 'shakespeare'),\n",
    "     ('shakespeare', '1'),\n",
    "     ('1', 'fairest'),\n",
    "     ('fairest', 'creatures'),\n",
    "     ('creatures', 'desire'),\n",
    "     ('desire', 'increase'),\n",
    "     ('increase', 'thereby'),\n",
    "     ('thereby', 'beauty'),\n",
    "     ('beauty', \"'s\"),\n",
    "     (\"'s\", 'rose'),\n",
    "     ('rose', 'might'),\n",
    "     ('might', 'never')]\n",
    "\n",
    ">```python\n",
    "train_bigrams[-15:]\n",
    "\n",
    "    [('prohibited', 'commercial'),\n",
    "     ('commercial', 'distribution'),\n",
    "     ('distribution', 'includes'),\n",
    "     ('includes', 'service'),\n",
    "     ('service', 'charges'),\n",
    "     ('charges', 'download'),\n",
    "     ('download', 'time'),\n",
    "     ('time', 'membership.'),\n",
    "     ('membership.', 'end'),\n",
    "     ('end', 'etext'),\n",
    "     ('etext', 'complete'),\n",
    "     ('complete', 'works'),\n",
    "     ('works', 'william'),\n",
    "     ('william', 'shakespeare'),\n",
    "     ('shakespeare', '</s>')]\n",
    "\n",
    ">```python\n",
    "train_fivegrams[0:15]\n",
    "\n",
    "    [('<s>', '<s>', '<s>', '<s>', '1609'),\n",
    "     ('<s>', '<s>', '<s>', '1609', 'sonnets'),\n",
    "     ('<s>', '<s>', '1609', 'sonnets', 'william'),\n",
    "     ('<s>', '1609', 'sonnets', 'william', 'shakespeare'),\n",
    "     ('1609', 'sonnets', 'william', 'shakespeare', '1'),\n",
    "     ('sonnets', 'william', 'shakespeare', '1', 'fairest'),\n",
    "     ('william', 'shakespeare', '1', 'fairest', 'creatures'),\n",
    "     ('shakespeare', '1', 'fairest', 'creatures', 'desire'),\n",
    "     ('1', 'fairest', 'creatures', 'desire', 'increase'),\n",
    "     ('fairest', 'creatures', 'desire', 'increase', 'thereby'),\n",
    "     ('creatures', 'desire', 'increase', 'thereby', 'beauty'),\n",
    "     ('desire', 'increase', 'thereby', 'beauty', \"'s\"),\n",
    "     ('increase', 'thereby', 'beauty', \"'s\", 'rose'),\n",
    "     ('thereby', 'beauty', \"'s\", 'rose', 'might'),\n",
    "     ('beauty', \"'s\", 'rose', 'might', 'never')]\n",
    "\n",
    ">```python\n",
    "train_fivegrams[-15:]\n",
    "\n",
    "    [('distributed', 'used', 'commercially', 'prohibited', 'commercial'),\n",
    "     ('used', 'commercially', 'prohibited', 'commercial', 'distribution'),\n",
    "     ('commercially', 'prohibited', 'commercial', 'distribution', 'includes'),\n",
    "     ('prohibited', 'commercial', 'distribution', 'includes', 'service'),\n",
    "     ('commercial', 'distribution', 'includes', 'service', 'charges'),\n",
    "     ('distribution', 'includes', 'service', 'charges', 'download'),\n",
    "     ('includes', 'service', 'charges', 'download', 'time'),\n",
    "     ('service', 'charges', 'download', 'time', 'membership.'),\n",
    "     ('charges', 'download', 'time', 'membership.', 'end'),\n",
    "     ('download', 'time', 'membership.', 'end', 'etext'),\n",
    "     ('time', 'membership.', 'end', 'etext', 'complete'),\n",
    "     ('membership.', 'end', 'etext', 'complete', 'works'),\n",
    "     ('end', 'etext', 'complete', 'works', 'william'),\n",
    "     ('etext', 'complete', 'works', 'william', 'shakespeare'),\n",
    "     ('complete', 'works', 'william', 'shakespeare', '</s>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5384bd91-01b3-40bf-b709-c2c57d7d817b",
   "metadata": {
    "id": "5384bd91-01b3-40bf-b709-c2c57d7d817b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bigrams[0:15]\n",
      "[('<s>', '1609'),\n",
      " ('1609', 'sonnets'),\n",
      " ('sonnets', 'william'),\n",
      " ('william', 'shakespeare'),\n",
      " ('shakespeare', '1'),\n",
      " ('1', 'fairest'),\n",
      " ('fairest', 'creatures'),\n",
      " ('creatures', 'desire'),\n",
      " ('desire', 'increase'),\n",
      " ('increase', 'thereby'),\n",
      " ('thereby', 'beauty'),\n",
      " ('beauty', \"'s\"),\n",
      " (\"'s\", 'rose'),\n",
      " ('rose', 'might'),\n",
      " ('might', 'never')]\n",
      "\n",
      "train_bigrams[-15:]\n",
      "[('prohibited', 'commercial'),\n",
      " ('commercial', 'distribution'),\n",
      " ('distribution', 'includes'),\n",
      " ('includes', 'service'),\n",
      " ('service', 'charges'),\n",
      " ('charges', 'download'),\n",
      " ('download', 'time'),\n",
      " ('time', 'membership.'),\n",
      " ('membership.', 'end'),\n",
      " ('end', 'etext'),\n",
      " ('etext', 'complete'),\n",
      " ('complete', 'works'),\n",
      " ('works', 'william'),\n",
      " ('william', 'shakespeare'),\n",
      " ('shakespeare', '</s>')]\n",
      "\n",
      "train_fivegrams[0:15]\n",
      "[('<s>', '<s>', '<s>', '<s>', '1609'),\n",
      " ('<s>', '<s>', '<s>', '1609', 'sonnets'),\n",
      " ('<s>', '<s>', '1609', 'sonnets', 'william'),\n",
      " ('<s>', '1609', 'sonnets', 'william', 'shakespeare'),\n",
      " ('1609', 'sonnets', 'william', 'shakespeare', '1'),\n",
      " ('sonnets', 'william', 'shakespeare', '1', 'fairest'),\n",
      " ('william', 'shakespeare', '1', 'fairest', 'creatures'),\n",
      " ('shakespeare', '1', 'fairest', 'creatures', 'desire'),\n",
      " ('1', 'fairest', 'creatures', 'desire', 'increase'),\n",
      " ('fairest', 'creatures', 'desire', 'increase', 'thereby'),\n",
      " ('creatures', 'desire', 'increase', 'thereby', 'beauty'),\n",
      " ('desire', 'increase', 'thereby', 'beauty', \"'s\"),\n",
      " ('increase', 'thereby', 'beauty', \"'s\", 'rose'),\n",
      " ('thereby', 'beauty', \"'s\", 'rose', 'might'),\n",
      " ('beauty', \"'s\", 'rose', 'might', 'never')]\n",
      "\n",
      "train_fivegrams[-15:]\n",
      "[('distributed', 'used', 'commercially', 'prohibited', 'commercial'),\n",
      " ('used', 'commercially', 'prohibited', 'commercial', 'distribution'),\n",
      " ('commercially', 'prohibited', 'commercial', 'distribution', 'includes'),\n",
      " ('prohibited', 'commercial', 'distribution', 'includes', 'service'),\n",
      " ('commercial', 'distribution', 'includes', 'service', 'charges'),\n",
      " ('distribution', 'includes', 'service', 'charges', 'download'),\n",
      " ('includes', 'service', 'charges', 'download', 'time'),\n",
      " ('service', 'charges', 'download', 'time', 'membership.'),\n",
      " ('charges', 'download', 'time', 'membership.', 'end'),\n",
      " ('download', 'time', 'membership.', 'end', 'etext'),\n",
      " ('time', 'membership.', 'end', 'etext', 'complete'),\n",
      " ('membership.', 'end', 'etext', 'complete', 'works'),\n",
      " ('end', 'etext', 'complete', 'works', 'william'),\n",
      " ('etext', 'complete', 'works', 'william', 'shakespeare'),\n",
      " ('complete', 'works', 'william', 'shakespeare', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "import pprint\n",
    "\n",
    "def create_ngrams(tokens, n):\n",
    "    start_tokens = ['<s>'] * (n - 1)\n",
    "    end_tokens = ['</s>']\n",
    "    padded_tokens = start_tokens + tokens + end_tokens\n",
    "    return list(ngrams(padded_tokens, n))\n",
    "\n",
    "try:\n",
    "    train_bigrams = create_ngrams(cleaned_train_text, 2)\n",
    "    train_fivegrams = create_ngrams(cleaned_train_text, 5)\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=1)\n",
    "\n",
    "    print(\"train_bigrams[0:15]\")\n",
    "    pp.pprint(train_bigrams[0:15])\n",
    "\n",
    "    print(\"\\ntrain_bigrams[-15:]\")\n",
    "    pp.pprint(train_bigrams[-15:])\n",
    "\n",
    "    print(\"\\ntrain_fivegrams[0:15]\")\n",
    "    pp.pprint(train_fivegrams[0:15])\n",
    "\n",
    "    print(\"\\ntrain_fivegrams[-15:]\")\n",
    "    pp.pprint(train_fivegrams[-15:])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443fb75-76b7-41eb-b52c-b97ecdc45133",
   "metadata": {
    "id": "0443fb75-76b7-41eb-b52c-b97ecdc45133"
   },
   "source": [
    "#### 1.5 - Build Vocabulary:\n",
    "\n",
    "Define a function called `build_vocab` that creates a set of unique words from a list of tokens, EXCLUDING the `<DELETED>` placeholder. The function should take the following parameter:\n",
    "\n",
    "- `tokens`: A **list** of clean tokens from which to build the vocabulary.\n",
    "    \n",
    "The function `build_vocab` should return a `set` of unique tokens, which will be our vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "Execute the `build_vocab` function using `cleaned_train_text` to construct a set of unique words, which will serve as the vocabulary for the N-gram model. Store the result in a variable named `vocab`, ensuring that the `<DELETED>` placeholder is not included in the vocabulary\n",
    "\n",
    "\n",
    "ps.: Trying to remove the `<DELETED>` placeholder from the train text will produce an error since this placeholder only exists in the test text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa46fe22-0ca0-46af-bf10-6a6cfe8185a2",
   "metadata": {
    "id": "aa46fe22-0ca0-46af-bf10-6a6cfe8185a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29331\n",
      "\n",
      "First 20 vocabulary items (sorted):\n",
      "[\"'-'god-a-mercy\", \"'-on\", \"'-why\", \"'abbominable\", \"'above\", \"'accommodated\", \"'accost\", \"'accurs'd\", \"'achilles\", \"'ad\", \"'adieu\", \"'affected\", \"'after\", \"'against\", \"'aged\", \"'agrippa\", \"'ah\", \"'aio\", \"'air\", \"'alack\"]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokens):\n",
    "    vocab = {token for token in tokens if token.lower() not in ['<deleted>', '<DELETED>']}\n",
    "    return vocab\n",
    "\n",
    "try:\n",
    "    vocab = build_vocab(cleaned_train_text)\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    print(\"\\nFirst 20 vocabulary items (sorted):\")\n",
    "    print(sorted(list(vocab))[:20])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a78bd-1d45-4c22-badf-284dbd2ebb30",
   "metadata": {
    "id": "b31a78bd-1d45-4c22-badf-284dbd2ebb30"
   },
   "source": [
    "### 2A - N-gram Model Training - part A\n",
    "\n",
    "Training an N-gram model is a key step in many natural language processing tasks. This process involves calculating the frequency distribution of N-grams and estimating their probabilities based on the training corpus. These statistics will then be used to predict the next word in a sequence or to determine the most likely correction in a text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cd41e-b194-4440-bbc6-1513e660f728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T20:00:12.890464Z",
     "iopub.status.busy": "2024-03-01T20:00:12.889463Z",
     "iopub.status.idle": "2024-03-01T20:00:12.911201Z",
     "shell.execute_reply": "2024-03-01T20:00:12.910189Z",
     "shell.execute_reply.started": "2024-03-01T20:00:12.890464Z"
    },
    "id": "bb4cd41e-b194-4440-bbc6-1513e660f728"
   },
   "source": [
    "#### 2A.1 - Import necessary libraries:\n",
    "\n",
    ">```python\n",
    "from nltk import FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008b76bc-1503-4bb6-8b2d-47641ae3601b",
   "metadata": {
    "id": "008b76bc-1503-4bb6-8b2d-47641ae3601b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram model training environment ready!\n",
      "Available frequency distribution classes:\n",
      "- FreqDist: For unigram frequencies\n",
      "- ConditionalFreqDist: For n-gram frequencies\n",
      "- ConditionalProbDist: For probability distributions\n",
      "- MLEProbDist: For maximum likelihood estimation\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "import nltk\n",
    "\n",
    "class NgramModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.unigram_freq = FreqDist()\n",
    "        self.bigram_cfd = ConditionalFreqDist()\n",
    "        self.trigram_cfd = ConditionalFreqDist()\n",
    "        self.fivegram_cfd = ConditionalFreqDist()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        trainer = NgramModelTrainer()\n",
    "\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "\n",
    "        print(\"N-gram model training environment ready!\")\n",
    "        print(\"Available frequency distribution classes:\")\n",
    "        print(\"- FreqDist: For unigram frequencies\")\n",
    "        print(\"- ConditionalFreqDist: For n-gram frequencies\")\n",
    "        print(\"- ConditionalProbDist: For probability distributions\")\n",
    "        print(\"- MLEProbDist: For maximum likelihood estimation\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during initialization: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397be122-e6eb-4332-90d3-987e01b94bf5",
   "metadata": {
    "id": "397be122-e6eb-4332-90d3-987e01b94bf5"
   },
   "source": [
    "#### 2A.2 - Calculate Frequency Distribution\n",
    "\n",
    "Create a function named `calculate_ngram_freq` to calculate the frequency distribution of N-grams in the training data, employing the `FreqDist` class from the `nltk` library. The function should take the following parameter:\n",
    "\n",
    "- `ngrams_list`: A list of N-grams for which to calculate the frequency distribution.\n",
    "\n",
    "The function should return:\n",
    "\n",
    "- A `FreqDist` object representing the Frequency Distribution of the input N-grams.\n",
    "\n",
    "Using the function `calculate_ngram_freq`, create two variables named `bigram_freq_dist` and `fivegram_freq_dist`. Use the appropriate N-grams list.\n",
    "\n",
    "\n",
    "**Expected Outuput:**\n",
    "\n",
    "\n",
    ">```python\n",
    "bigram_freq_dist\n",
    "\n",
    "    FreqDist({('thou', 'art'): 543, ('king', 'henry'): 402, ('thou', 'hast'): 369, ('exeunt', 'scene'): 341, ('king', 'richard'): 278, ('let', 'us'): 259, ('william', 'shakespeare'): 257, ('let', \"'s\"): 255, ('art', 'thou'): 234, ('thou', 'shalt'): 233, ...})\n",
    "\n",
    "\n",
    ">```python\n",
    "fivegram_freq_dist\n",
    "\n",
    "    FreqDist({('electronic', 'version', 'complete', 'works', 'william'): 218, ('version', 'complete', 'works', 'william', 'shakespeare'): 218, ('complete', 'works', 'william', 'shakespeare', 'copyright'): 218, ('works', 'william', 'shakespeare', 'copyright', '1990-1993'): 218, ('william', 'shakespeare', 'copyright', '1990-1993', 'world'): 218, ('shakespeare', 'copyright', '1990-1993', 'world', 'library'): 218, ('copyright', '1990-1993', 'world', 'library', 'inc.'): 218, ('1990-1993', 'world', 'library', 'inc.', 'provided'): 218, ('world', 'library', 'inc.', 'provided', 'project'): 218, ('library', 'inc.', 'provided', 'project', 'gutenberg'): 218, ...})\n",
    "\n",
    "\n",
    "**References**\n",
    "[FreqDist](https://www.nltk.org/api/nltk.probability.FreqDist.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff06292b-4cc7-4752-a29b-53795d8902d3",
   "metadata": {
    "id": "ff06292b-4cc7-4752-a29b-53795d8902d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bigram_freq_dist\n",
      "FreqDist({('thou', 'art'): 543, ('king', 'henry'): 402, ('thou', 'hast'): 369, ('exeunt', 'scene'): 341, ('king', 'richard'): 278, ('let', 'us'): 259, ('william', 'shakespeare'): 257, ('let', \"'s\"): 255, ('art', 'thou'): 234, ('thou', 'shalt'): 233})\n",
      "\n",
      "fivegram_freq_dist\n",
      "FreqDist({('electronic', 'version', 'complete', 'works', 'william'): 218, ('version', 'complete', 'works', 'william', 'shakespeare'): 218, ('complete', 'works', 'william', 'shakespeare', 'copyright'): 218, ('works', 'william', 'shakespeare', 'copyright', '1990-1993'): 218, ('william', 'shakespeare', 'copyright', '1990-1993', 'world'): 218, ('shakespeare', 'copyright', '1990-1993', 'world', 'library'): 218, ('copyright', '1990-1993', 'world', 'library', 'inc.'): 218, ('1990-1993', 'world', 'library', 'inc.', 'provided'): 218, ('world', 'library', 'inc.', 'provided', 'project'): 218, ('library', 'inc.', 'provided', 'project', 'gutenberg'): 218})\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def calculate_ngram_freq(ngrams_list):\n",
    "    try:\n",
    "        freq_dist = FreqDist(ngrams_list)\n",
    "        return freq_dist\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating n-gram frequencies: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    bigram_freq_dist = calculate_ngram_freq(train_bigrams)\n",
    "    fivegram_freq_dist = calculate_ngram_freq(train_fivegrams)\n",
    "\n",
    "    print(\"\\nbigram_freq_dist\")\n",
    "    print(f\"FreqDist({dict(bigram_freq_dist.most_common(10))})\")\n",
    "\n",
    "    print(\"\\nfivegram_freq_dist\")\n",
    "    print(f\"FreqDist({dict(fivegram_freq_dist.most_common(10))})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in main execution: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549782d-8d5d-4826-b0eb-d097c870049b",
   "metadata": {
    "id": "5549782d-8d5d-4826-b0eb-d097c870049b"
   },
   "source": [
    "#### 2A.3 - Probability Estimation\n",
    "\n",
    "Create a function named `estimate_ngram_probabilities` to estimate the conditional probabilities of N-grams, utilizing the `ConditionalFreqDist` and `ConditionalProbDist` classes along with a probability distribution such as `MLEProbDist` from the `nltk.probability` module. The function should take the following parameter:\n",
    "    \n",
    "- `ngrams_list`: A list of N-grams for which to estimate conditional probabilities.\n",
    "   \n",
    "The function should return:\n",
    "\n",
    "- A `ConditionalProbDist` object representing the conditional probabilities of the input N-grams.\n",
    "\n",
    "\n",
    "Using the function `Probability Estimation `, create two variables named `bigram_prob_dist` and `fivegram_prob_dist`. Use the appropriate N-grams list.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "[ConditionalProbDist](https://tedboy.github.io/nlps/generated/generated/nltk.ConditionalProbDist.html)\n",
    "\n",
    "[MLEProbDist](https://www.nltk.org/api/nltk.probability.MLEProbDist.html?highlight=probability+probdist#nltk.probability.MLEProbDist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccdb45bd-a2f5-41b6-96d1-60e3d2904fe7",
   "metadata": {
    "id": "ccdb45bd-a2f5-41b6-96d1-60e3d2904fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Bigram Probabilities:\n",
      "\n",
      "Probabilities following '('thou',)':\n",
      "P('art' | '('thou',)') = 0.0992\n",
      "P('hast' | '('thou',)') = 0.0674\n",
      "P('shalt' | '('thou',)') = 0.0426\n",
      "\n",
      "Example Fivegram Probabilities:\n",
      "\n",
      "Probabilities following ('electronic', 'version', 'complete', 'works'):\n",
      "P('william' | ('electronic', 'version', 'complete', 'works')) = 1.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk import ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "\n",
    "def estimate_ngram_probabilities(ngrams_list):\n",
    "    try:\n",
    "        cfd = ConditionalFreqDist(\n",
    "            (tuple(ngram[:-1]), ngram[-1]) \n",
    "            for ngram in ngrams_list\n",
    "        )\n",
    "        \n",
    "        cpd = ConditionalProbDist(cfd, MLEProbDist)\n",
    "        \n",
    "        return cpd\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error estimating n-gram probabilities: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    bigram_prob_dist = estimate_ngram_probabilities(train_bigrams)\n",
    "    fivegram_prob_dist = estimate_ngram_probabilities(train_fivegrams)\n",
    "\n",
    "    print(\"\\nExample Bigram Probabilities:\")\n",
    "    context = ('thou',)\n",
    "    if context in bigram_prob_dist.conditions():\n",
    "        print(f\"\\nProbabilities following '{context}':\")\n",
    "        for outcome in ['art', 'hast', 'shalt']:\n",
    "            prob = bigram_prob_dist[context].prob(outcome)\n",
    "            print(f\"P('{outcome}' | '{context}') = {prob:.4f}\")\n",
    "\n",
    "    print(\"\\nExample Fivegram Probabilities:\")\n",
    "    context = ('electronic', 'version', 'complete', 'works')\n",
    "    if context in fivegram_prob_dist.conditions():\n",
    "        print(f\"\\nProbabilities following {context}:\")\n",
    "        prob = fivegram_prob_dist[context].prob('william')\n",
    "        print(f\"P('william' | {context}) = {prob:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in main execution: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07086b93-8715-467d-8dd8-d0ffa00b8664",
   "metadata": {
    "id": "07086b93-8715-467d-8dd8-d0ffa00b8664"
   },
   "source": [
    "### 2B - N-gram Model Training - part B\n",
    "\n",
    "In this section we will create the core function of the ngram NLP: the function `predict_next_word`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f337c-7f13-4fe9-883f-eb50f25b4653",
   "metadata": {
    "id": "d87f337c-7f13-4fe9-883f-eb50f25b4653"
   },
   "source": [
    "#### 2A.1 - Import necessary libraries:\n",
    "\n",
    ">```python\n",
    "from nltk.probability import ConditionalProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c249b2-aaa4-4f01-96c6-8f6b1fc76fa6",
   "metadata": {
    "id": "e9c249b2-aaa4-4f01-96c6-8f6b1fc76fa6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word Predictor initialized with:\n",
      "- ConditionalProbDist support for probability distributions\n",
      "- Support for bigram and fivegram models\n",
      "Ready for model loading and prediction tasks\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import ConditionalProbDist\n",
    "from nltk import FreqDist, ConditionalFreqDist, MLEProbDist\n",
    "\n",
    "class NextWordPredictor:\n",
    "    def __init__(self):\n",
    "        self.bigram_prob_dist = None\n",
    "        self.fivegram_prob_dist = None\n",
    "        \n",
    "    def load_models(self, bigram_prob_dist, fivegram_prob_dist):\n",
    "        self.bigram_prob_dist = bigram_prob_dist\n",
    "        self.fivegram_prob_dist = fivegram_prob_dist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = NextWordPredictor()\n",
    "    \n",
    "    print(\"Next Word Predictor initialized with:\")\n",
    "    print(\"- ConditionalProbDist support for probability distributions\")\n",
    "    print(\"- Support for bigram and fivegram models\")\n",
    "    print(\"Ready for model loading and prediction tasks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c0230-7792-4173-b8fa-9677c9448f60",
   "metadata": {
    "id": "005c0230-7792-4173-b8fa-9677c9448f60"
   },
   "source": [
    "#### 2A.2 - Predict next word\n",
    "\n",
    "Create a function named `predict_next_word` that utilizes the conditional probabilities to predict the most probable next word after a given context.\n",
    "\n",
    "The function should take the following parameters:\n",
    "\n",
    "- A context, which is a tuple of words that precedes the word to be predicted. The size of the context should be N-1 for an N-gram model.\n",
    "- A `ConditionalProbDist` object that has been previously computed from the training data.\n",
    "- Optionally accepts an integer `top_n` that specifies the number of top probable next words to return (default is 1, which returns the most probable next word).\n",
    "\n",
    "\n",
    "The function should handle cases where the context is not found in the `ConditionalProbDist`, returning the default value `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be315d11-9383-45a9-8940-4985ab70b371",
   "metadata": {
    "id": "be315d11-9383-45a9-8940-4985ab70b371",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for context ('thou',):\n",
      "Word: 'art', Probability: 0.0992\n",
      "Word: 'hast', Probability: 0.0674\n",
      "Word: 'shalt', Probability: 0.0426\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(context, cpd, top_n=1):\n",
    "    try:\n",
    "        if context not in cpd.conditions():\n",
    "            return [('<UNK>', 0.0)]\n",
    "        \n",
    "        prob_dist = cpd[context]\n",
    "        \n",
    "        next_words = [(word, prob_dist.prob(word)) for word in prob_dist.samples()]\n",
    "        \n",
    "        next_words.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return next_words[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting next word: {str(e)}\")\n",
    "        return [('<UNK>', 0.0)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_context = ('thou',)\n",
    "    try:\n",
    "        predictions = predict_next_word(example_context, bigram_prob_dist, top_n=3)\n",
    "        \n",
    "        print(f\"\\nPredictions for context {example_context}:\")\n",
    "        for word, prob in predictions:\n",
    "            print(f\"Word: '{word}', Probability: {prob:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in example: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd363f-9d3b-45df-bd46-c0c7dc47bb54",
   "metadata": {
    "id": "e8dd363f-9d3b-45df-bd46-c0c7dc47bb54"
   },
   "source": [
    "### 3 - Text Correction\n",
    "\n",
    "After training our N-gram model, the next step is to apply it to correct texts that contain placeholders indicating missing words. In this section, we will import the test text and use our model to predict the words that should replace the `<DELETED>` placeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281db13-98ac-407c-8c19-36beb1b7b631",
   "metadata": {
    "id": "7281db13-98ac-407c-8c19-36beb1b7b631"
   },
   "source": [
    "#### 3.1 - Correction Function\n",
    "\n",
    "Create a function called `correct_text_with_ngrams` that searches for `<DELETED>` placeholders in the test data and uses the `predict_next_word` function to find the most probable replacement.\n",
    "\n",
    "The function should take the following parameters:\n",
    "\n",
    "- `text_data`: The list of tokens from the test data, including `<DELETED>` placeholders.\n",
    "- `ngram_model`: The trained N-gram model to use for prediction (e.g., bigram or fivegram model).\n",
    "- `n`: The order of the N-gram (e.g., 2 for bigrams, 3 for trigrams, etc.).\n",
    "\n",
    "The function should return:\n",
    "\n",
    "- `corrected_text`: A **list** of tokens where `<DELETED>` placeholders have been replaced with the most probable word predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65bb845d-a034-4b6d-aeb1-7cac45125409",
   "metadata": {
    "id": "65bb845d-a034-4b6d-aeb1-7cac45125409",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram correction:\n",
      "Original: in fair verona where we <DELETED> our scene\n",
      "Corrected: in fair verona where we <UNK> our scene\n",
      "\n",
      "Fivegram correction:\n",
      "Original: in fair verona where we <DELETED> our scene\n",
      "Corrected: in fair verona where we <UNK> our scene\n"
     ]
    }
   ],
   "source": [
    "def correct_text_with_ngrams(text_data, cpd, n):\n",
    "    corrected_text = text_data.copy()\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(corrected_text)):\n",
    "            if corrected_text[i] == '<DELETED>':\n",
    "                start_idx = max(0, i - (n - 1))\n",
    "                context = tuple(corrected_text[start_idx:i])\n",
    "                \n",
    "                if len(context) < n - 1:\n",
    "                    context = ('<s>,') * (n - 1 - len(context)) + context\n",
    "                \n",
    "                prediction = predict_next_word(context, cpd)[0]\n",
    "                \n",
    "                corrected_text[i] = prediction[0]\n",
    "        \n",
    "        return corrected_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error correcting text: {str(e)}\")\n",
    "        return text_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_text = ['in', 'fair', 'verona', 'where', 'we', '<DELETED>', 'our', 'scene']\n",
    "    \n",
    "    try:\n",
    "        corrected_bigram = correct_text_with_ngrams(test_text, bigram_prob_dist, 2)\n",
    "        print(\"\\nBigram correction:\")\n",
    "        print(f\"Original: {' '.join(test_text)}\")\n",
    "        print(f\"Corrected: {' '.join(corrected_bigram)}\")\n",
    "        \n",
    "        corrected_fivegram = correct_text_with_ngrams(test_text, fivegram_prob_dist, 5)\n",
    "        print(\"\\nFivegram correction:\")\n",
    "        print(f\"Original: {' '.join(test_text)}\")\n",
    "        print(f\"Corrected: {' '.join(corrected_fivegram)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in example: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d567c1-0c23-4f85-9012-b8b954fe5a5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T20:32:23.975490Z",
     "iopub.status.busy": "2024-03-01T20:32:23.974493Z",
     "iopub.status.idle": "2024-03-01T20:32:23.995597Z",
     "shell.execute_reply": "2024-03-01T20:32:23.994651Z",
     "shell.execute_reply.started": "2024-03-01T20:32:23.975490Z"
    },
    "id": "17d567c1-0c23-4f85-9012-b8b954fe5a5f"
   },
   "source": [
    "#### 3.2 - Load and Clean Test Data\n",
    "\n",
    "Import the `WS_test.txt` file using the `load_text` function. Then, apply the `clean_text` function to prepare the data for correction.\n",
    "\n",
    "Execute the `load_text` function to import the content of `WS_test.txt` and store it in a variable named `test_text`.\n",
    "Apply `clean_text` to `test_text` to obtain a tokenized and cleaned list of words, including `<DELETED>` placeholders, and store it in a variable named `cleaned_test_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86487111-476f-4c90-97b5-f7729f8987d3",
   "metadata": {
    "id": "86487111-476f-4c90-97b5-f7729f8987d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data Statistics:\n",
      "Total tokens in test text: 46857\n",
      "Number of <DELETED> placeholders: 0\n",
      "\n",
      "Sample of cleaned test text (first 50 tokens):\n",
      "till nobles armed commons take thou vial bed let nurse lie thee thy chamber provided project gutenberg etext illinois benedictine college lady percy get ground deleted king moth behold sun-beamed eyes- commercially prohibited commercial distribution includes speak language 't may grow sprout high heaven recordation noble husband shall stiff stark\n",
      "\n",
      "Example placeholders in context:\n"
     ]
    }
   ],
   "source": [
    "def load_and_clean_test_data():\n",
    "    try:\n",
    "        test_text = load_text('WS_test.txt')\n",
    "        cleaned_test_text = clean_text(test_text)\n",
    "\n",
    "        print(\"\\nTest Data Statistics:\")\n",
    "        print(f\"Total tokens in test text: {len(cleaned_test_text)}\")\n",
    "        print(f\"Number of <DELETED> placeholders: {cleaned_test_text.count('<DELETED>')}\")\n",
    "\n",
    "        return test_text, cleaned_test_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        test_text, cleaned_test_text = load_and_clean_test_data()\n",
    "        \n",
    "        if cleaned_test_text:\n",
    "            print(\"\\nSample of cleaned test text (first 50 tokens):\")\n",
    "            print(' '.join(cleaned_test_text[:50]))\n",
    "            \n",
    "            print(\"\\nExample placeholders in context:\")\n",
    "            for i, token in enumerate(cleaned_test_text):\n",
    "                if token == '<DELETED>' and i > 2 and i < len(cleaned_test_text) - 3:\n",
    "                    context = ' '.join(cleaned_test_text[i-3:i+4])\n",
    "                    print(f\"Context: ...{context}...\")\n",
    "                    if i > 10:\n",
    "                        break\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d986ba-bb8c-4a00-9a49-3a2d89543c41",
   "metadata": {
    "id": "75d986ba-bb8c-4a00-9a49-3a2d89543c41"
   },
   "source": [
    "#### 3.3 Applying Bigram Model\n",
    "\n",
    "Apply the `correct_text_with_ngrams` function to the `cleaned_test_text` using the bigram `bigram_prob_dist` object created before. Save the output to a variable named `corrected_test_text_bigram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "835f26a3-1189-4f40-9e76-217b59c241ba",
   "metadata": {
    "id": "835f26a3-1189-4f40-9e76-217b59c241ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram Model Correction Results:\n",
      "Original text length: 46857 tokens\n",
      "Corrected text length: 46857 tokens\n",
      "\n",
      "Example corrections (original -> corrected):\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    corrected_test_text_bigram = correct_text_with_ngrams(cleaned_test_text, bigram_prob_dist, 2)\n",
    "\n",
    "    print(\"\\nBigram Model Correction Results:\")\n",
    "    print(f\"Original text length: {len(cleaned_test_text)} tokens\")\n",
    "    print(f\"Corrected text length: {len(corrected_test_text_bigram)} tokens\")\n",
    "\n",
    "    print(\"\\nExample corrections (original -> corrected):\")\n",
    "    for i, (orig, corr) in enumerate(zip(cleaned_test_text, corrected_test_text_bigram)):\n",
    "        if orig == '<DELETED>':\n",
    "            start = max(0, i - 3)\n",
    "            end = min(len(cleaned_test_text), i + 4)\n",
    "            orig_context = ' '.join(cleaned_test_text[start:end])\n",
    "            corr_context = ' '.join(corrected_test_text_bigram[start:end])\n",
    "            print(f\"\\nOriginal:  ...{orig_context}...\")\n",
    "            print(f\"Corrected: ...{corr_context}...\")\n",
    "            if i > 20:\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error applying bigram correction: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6f978-6c90-47da-8f2d-69c44409d23d",
   "metadata": {
    "id": "99c6f978-6c90-47da-8f2d-69c44409d23d"
   },
   "source": [
    "#### 3.4 Applying Fivegram Model\n",
    "\n",
    "Apply the `correct_text_with_ngrams` function to the `cleaned_test_text` using the bigram `fivegram_prob_dist` object created before. Save the output to a variable named `corrected_test_text_fivegram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "812f6820-16e6-4197-a0aa-9836ea6d8fd2",
   "metadata": {
    "id": "812f6820-16e6-4197-a0aa-9836ea6d8fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fivegram Model Correction Results:\n",
      "Original text length: 46857 tokens\n",
      "Corrected text length: 46857 tokens\n",
      "\n",
      "Example corrections (original -> corrected):\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    corrected_test_text_fivegram = correct_text_with_ngrams(cleaned_test_text, fivegram_prob_dist, 5)\n",
    "\n",
    "    print(\"\\nFivegram Model Correction Results:\")\n",
    "    print(f\"Original text length: {len(cleaned_test_text)} tokens\")\n",
    "    print(f\"Corrected text length: {len(corrected_test_text_fivegram)} tokens\")\n",
    "\n",
    "    print(\"\\nExample corrections (original -> corrected):\")\n",
    "    for i, (orig, corr) in enumerate(zip(cleaned_test_text, corrected_test_text_fivegram)):\n",
    "        if orig == '<DELETED>':\n",
    "            start = max(0, i - 5)\n",
    "            end = min(len(cleaned_test_text), i + 6)\n",
    "            orig_context = ' '.join(cleaned_test_text[start:end])\n",
    "            corr_context = ' '.join(corrected_test_text_fivegram[start:end])\n",
    "            print(f\"\\nOriginal:  ...{orig_context}...\")\n",
    "            print(f\"Corrected: ...{corr_context}...\")\n",
    "            if i > 20:\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error applying fivegram correction: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b782631-daa6-46f9-94bf-502584650e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T02:33:01.664693Z",
     "iopub.status.busy": "2023-11-07T02:33:01.664693Z",
     "iopub.status.idle": "2023-11-07T02:33:01.675715Z",
     "shell.execute_reply": "2023-11-07T02:33:01.674704Z",
     "shell.execute_reply.started": "2023-11-07T02:33:01.664693Z"
    },
    "id": "0b782631-daa6-46f9-94bf-502584650e01",
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluating the performance of our N-gram model is crucial to understanding its effectiveness in text correction tasks. In this section, we will calculate the accuracy of our model by comparing the predicted text against a validation text that contains the correct words.\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "1. **Import and Clean Validation Data**: Load and preprocess the `WS_validation.txt` file to obtain a clean list of tokens for accuracy comparison.\n",
    "\n",
    "2. **Accuracy Calculation**: Use the supplied function `calculate_accuracy` to calculate the accurary for both models.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "1. **Load Validation Text**: Use the `load_text` function to import the `WS_validation.txt` file.\n",
    "\n",
    "2. **Clean Validation Text**: Apply the `clean_text` function to the imported validation text to produce a list of clean tokens for comparison.\n",
    "\n",
    "3. **Accuracy Calculation**: This function takes:\n",
    "\n",
    "   - `test_tokens`: A list of tokes with the `<DELETED>` placeholder (before correction).\n",
    "   - `corrected_tokens`: A list of tokens that have been corrected by the N-gram model (either bigram or fivegram).\n",
    "   - `validation_tokens`: A list of clean tokens from the validation text.<B></B>\n",
    "   \n",
    "   The function should return the accuracy as a float, calculated as the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "#### Steps to Follow:\n",
    "\n",
    "1. **Import and Clean Validation Data**:\n",
    "\n",
    "   - Use the `load_text` function to load the `WS_validation.txt` as the validation text data. Store it as `validation_text`.\n",
    "   - Apply the `clean_text` function to the loaded `validation_text` to obtain a list of tokens for accuracy comparison, named `cleaned_validation_text`.<B></B>\n",
    "\n",
    "2. **Accuracy Calculation**:\n",
    "\n",
    "   - Store the resulting accuracy scores in variables named `bigram_accuracy` and `fivegram_accuracy`, respectively.<B></B>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3f960b1-d697-45cb-b72e-99b9b25f50fa",
   "metadata": {
    "id": "b3f960b1-d697-45cb-b72e-99b9b25f50fa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning texts...\n",
      "\n",
      "Initial text lengths:\n",
      "Test text: 85187 tokens\n",
      "Validation text: 85187 tokens\n",
      "\n",
      "Applying n-gram corrections...\n",
      "\n",
      "Found 1740 <DELETED> tokens to evaluate\n",
      "\n",
      "Model Evaluation Results:\n",
      "----------------------------------------\n",
      "Bigram Model Accuracy:   0.0448\n",
      "Fivegram Model Accuracy: 0.0178\n",
      "\n",
      "Example Predictions:\n",
      "----------------------------------------\n",
      "\n",
      "Context: ...get ground and <DELETED> of the king...\n",
      "Correct word:     vantage\n",
      "Bigram predict:   <UNK>\n",
      "Fivegram predict: <UNK>\n",
      "\n",
      "Context: ...stark and cold <DELETED> like death each...\n",
      "Correct word:     appear\n",
      "Bigram predict:   blood\n",
      "Fivegram predict: <UNK>\n",
      "\n",
      "Context: ...time against thou <DELETED> awake and this...\n",
      "Correct word:     shalt\n",
      "Bigram predict:   art\n",
      "Fivegram predict: <UNK>\n",
      "\n",
      "Context: ...thee from this <DELETED> shame knowest sir...\n",
      "Correct word:     present\n",
      "Bigram predict:   <UNK>\n",
      "Fivegram predict: <UNK>\n",
      "\n",
      "Context: ...dry round old <DELETED> knights it angred...\n",
      "Correct word:     withered\n",
      "Bigram predict:   man\n",
      "Fivegram predict: withered\n",
      "\n",
      "Debug Information:\n",
      "Number of <DELETED> tokens: 1740\n",
      "Test text sample (first 50 tokens):\n",
      "till that the nobles and the armed commons take thou this vial being then in bed let not the nurse lie with thee in thy chamber provided by project gutenberg etext of illinois benedictine college lady percy if they get ground and <DELETED> of the king moth once to behold\n",
      "\n",
      "Validation text sample (first 50 tokens):\n",
      "till that the nobles and the armed commons take thou this vial being then in bed let not the nurse lie with thee in thy chamber provided by project gutenberg etext of illinois benedictine college lady percy if they get ground and vantage of the king moth once to behold\n"
     ]
    }
   ],
   "source": [
    "def clean_text_for_evaluation(text):\n",
    "    tokens = text.lower().split()\n",
    "    cleaned_tokens = [\n",
    "        '<DELETED>' if token in ['<deleted>', '<DELETED>'] else ''.join(c for c in token if c.isalnum())\n",
    "        for token in tokens if token in ['<deleted>', '<DELETED>'] or ''.join(c for c in token if c.isalnum())\n",
    "    ]\n",
    "    return cleaned_tokens\n",
    "\n",
    "try:\n",
    "    print(\"Loading and cleaning texts...\")\n",
    "    \n",
    "    test_text = load_text('WS_test.txt')\n",
    "    cleaned_test_text = clean_text_for_evaluation(test_text)\n",
    "    \n",
    "    validation_text = load_text('WS_validation.txt')\n",
    "    cleaned_validation_text = clean_text_for_evaluation(validation_text)\n",
    "    \n",
    "    print(f\"\\nInitial text lengths:\")\n",
    "    print(f\"Test text: {len(cleaned_test_text)} tokens\")\n",
    "    print(f\"Validation text: {len(cleaned_validation_text)} tokens\")\n",
    "    \n",
    "    print(\"\\nApplying n-gram corrections...\")\n",
    "    corrected_test_text_bigram = correct_text_with_ngrams(cleaned_test_text, bigram_prob_dist, 2)\n",
    "    corrected_test_text_fivegram = correct_text_with_ngrams(cleaned_test_text, fivegram_prob_dist, 5)\n",
    "    \n",
    "    deleted_positions = [i for i, token in enumerate(cleaned_test_text) if token == '<DELETED>']\n",
    "    total_deleted = len(deleted_positions)\n",
    "    \n",
    "    print(f\"\\nFound {total_deleted} <DELETED> tokens to evaluate\")\n",
    "    \n",
    "    bigram_correct = sum(\n",
    "        1 for pos in deleted_positions\n",
    "        if pos < len(cleaned_validation_text) and pos < len(corrected_test_text_bigram)\n",
    "        and corrected_test_text_bigram[pos] == cleaned_validation_text[pos]\n",
    "    )\n",
    "\n",
    "    fivegram_correct = sum(\n",
    "        1 for pos in deleted_positions\n",
    "        if pos < len(cleaned_validation_text) and pos < len(corrected_test_text_fivegram)\n",
    "        and corrected_test_text_fivegram[pos] == cleaned_validation_text[pos]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Bigram Model Accuracy:   {bigram_correct/total_deleted:.4f}\")\n",
    "    print(f\"Fivegram Model Accuracy: {fivegram_correct/total_deleted:.4f}\")\n",
    "    \n",
    "    print(\"\\nExample Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    for pos in deleted_positions[:5]:  \n",
    "        context_start = max(0, pos - 3)\n",
    "        context_end = min(len(cleaned_test_text), pos + 4)\n",
    "        context = ' '.join(cleaned_test_text[context_start:context_end])\n",
    "        \n",
    "        print(f\"\\nContext: ...{context}...\")\n",
    "        print(f\"Correct word:     {cleaned_validation_text[pos]}\")\n",
    "        print(f\"Bigram predict:   {corrected_test_text_bigram[pos]}\")\n",
    "        print(f\"Fivegram predict: {corrected_test_text_fivegram[pos]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in evaluation: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    \n",
    "print(\"\\nDebug Information:\")\n",
    "print(f\"Number of <DELETED> tokens: {sum(1 for t in cleaned_test_text if t == '<DELETED>')}\")\n",
    "print(f\"Test text sample (first 50 tokens):\")\n",
    "print(' '.join(cleaned_test_text[:50]))\n",
    "print(f\"\\nValidation text sample (first 50 tokens):\")\n",
    "print(' '.join(cleaned_validation_text[:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e846d-a59e-4fff-8786-4be6239f26c3",
   "metadata": {
    "id": "9e2e846d-a59e-4fff-8786-4be6239f26c3"
   },
   "source": [
    "### Export Models for codegrade evaluation\n",
    "\n",
    "Using the \"pickle\" library:\n",
    "\n",
    "- Export the model `unigram_model` as \"unigram_model_japanese.pkl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "474ae79c-9265-42df-ae07-02363f042162",
   "metadata": {
    "id": "474ae79c-9265-42df-ae07-02363f042162",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting unigram model...\n",
      "Successfully exported unigram model to 'unigram_model_japanese.pkl'\n",
      "\n",
      "Exporting fivegram model...\n",
      "Successfully exported fivegram model to 'fivegram_prob_dist.pkl'\n",
      "\n",
      "Verifying exported files:\n",
      "unigram_model_japanese.pkl exists: True\n",
      "fivegram_prob_dist.pkl exists: True\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return [''.join(char for char in token if char not in string.punctuation) for token in tokens if ''.join(char for char in token if char not in string.punctuation)]\n",
    "\n",
    "text = \"\"\"Your Japanese text here\"\"\"  \n",
    "\n",
    "tokens = clean_text(text)\n",
    "\n",
    "def create_unigram_model(tokens):\n",
    "    unigram_counts = Counter(tokens)\n",
    "    total_tokens = sum(unigram_counts.values())\n",
    "    return {word: count / total_tokens for word, count in unigram_counts.items()}\n",
    "\n",
    "unigram_model = create_unigram_model(tokens)\n",
    "\n",
    "try:\n",
    "    print(\"Exporting unigram model...\")\n",
    "    with open('unigram_model_japanese.pkl', 'wb') as file:\n",
    "        pickle.dump(unigram_model, file)\n",
    "    print(\"Successfully exported unigram model to 'unigram_model_japanese.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting unigram model: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nExporting fivegram model...\")\n",
    "    with open('fivegram_prob_dist.pkl', 'wb') as file:\n",
    "        pickle.dump(fivegram_prob_dist, file)\n",
    "    print(\"Successfully exported fivegram model to 'fivegram_prob_dist.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting fivegram model: {str(e)}\")\n",
    "\n",
    "print(\"\\nVerifying exported files:\")\n",
    "print(f\"unigram_model_japanese.pkl exists: {os.path.exists('unigram_model_japanese.pkl')}\")\n",
    "print(f\"fivegram_prob_dist.pkl exists: {os.path.exists('fivegram_prob_dist.pkl')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d93a10-c36b-46ab-9930-0d479bccbfe5",
   "metadata": {
    "id": "89d93a10-c36b-46ab-9930-0d479bccbfe5"
   },
   "source": [
    "This material is for enrolled students' academic use only and protected under U.S. Copyright Laws. This content must not be shared outside the confines of this course, in line with Eastern University's academic integrity policies. Unauthorized reproduction, distribution, or transmission of this material, including but not limited to posting on third-party platforms like GitHub, is strictly prohibited and may lead to disciplinary action. You may not alter or remove any copyright or other notice from copies of any content taken from BrightSpace or Eastern University’s website.\n",
    "\n",
    "© Copyright Notice 2024, Eastern University - All Rights Reserved"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
